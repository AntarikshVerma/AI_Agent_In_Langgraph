{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyM25ZqKw0NN/5FVDEmYgMjg",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/AntarikshVerma/AI_Agent_In_Langgraph/blob/main/Langgraph_agent.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# !pip install tavily-python==0.3.3\n",
        "# !pip install duckduckgo_search==5.3.1b1\n",
        "# !pip install beautifulsoup4==4.12.3\n",
        "# !pip install openai==1.30.1\n",
        "# !pip install langgraph==0.0.53\n",
        "# !pip install langchain_core==0.2.0\n",
        "# !pip install langchain_openai==0.1.7\n",
        "# !pip install langchain-community==0.2.0\n",
        "# !pip install langchain==0.2.0\n",
        "# !pip install aiosqlite==0.20.0\n",
        "# !pip install pygments==2.17.2\n",
        "# !pip install pygraphviz==1.13\n",
        "# !pip install gradio==4.31.3\n",
        "# !apt install libgraphviz-dev\n",
        "# !pip install pygraphviz\n"
      ],
      "metadata": {
        "id": "Vl1W0MZ4R8Fw"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "from google.colab import userdata\n",
        "open_ai_key=userdata.get('OPENAI_API_KEY')\n",
        "open_ai_base=userdata.get('OPEN_API_URL')\n",
        "open_ai_base=userdata.get('OPEN_API_URL')\n",
        "tavili_api_key=userdata.get('TAVILY_API_KEY')\n",
        "api_version=\"2023-07-01-preview\"\n",
        "\n",
        "os.environ[\"OPENAI_API_VERSION\"] = \"2023-12-01-preview\"\n",
        "os.environ[\"AZURE_OPENAI_ENDPOINT\"] = open_ai_base\n",
        "os.environ[\"AZURE_OPENAI_API_KEY\"] = open_ai_key\n",
        "os.environ[\"TAVILY_API_KEY\"]=tavili_api_key"
      ],
      "metadata": {
        "id": "KtAlFpVkZAST"
      },
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "id": "bBtTb8XFQs8M"
      },
      "outputs": [],
      "source": [
        "from langgraph.graph import StateGraph, END\n",
        "from typing import TypedDict, Annotated\n",
        "import operator\n",
        "from langchain_core.messages import AnyMessage, SystemMessage, HumanMessage, ToolMessage\n",
        "from langchain_openai import ChatOpenAI\n",
        "from langchain_community.tools.tavily_search import TavilySearchResults\n",
        "from langchain_openai import AzureOpenAI"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "llm_gpt_4 = AzureOpenAI(\n",
        "    deployment_name=\"gpt-4\",\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BRZo4O2XRbKN",
        "outputId": "227fc91b-7ec5-4ea6-81b8-ec674215713f"
      },
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1mAzureOpenAI\u001b[0m\n",
            "Params: {'deployment_name': 'gpt-4', 'model_name': 'gpt-3.5-turbo-instruct', 'temperature': 0.7, 'top_p': 1, 'frequency_penalty': 0, 'presence_penalty': 0, 'n': 1, 'logit_bias': {}, 'max_tokens': 256}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tool = TavilySearchResults(max_results=4) #increased number of results\n",
        "print(type(tool))\n",
        "print(tool.name)"
      ],
      "metadata": {
        "id": "TshWReGuWdDF",
        "outputId": "53b991d0-f034-44f4-9a91-b44382d7c082",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<class 'langchain_community.tools.tavily_search.tool.TavilySearchResults'>\n",
            "tavily_search_results_json\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class AgentState(TypedDict):\n",
        "    messages: Annotated[list[AnyMessage], operator.add]"
      ],
      "metadata": {
        "id": "E_jP4bP1WxJ_"
      },
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Agent:\n",
        "\n",
        "    def __init__(self, model, tools, system=\"\"):\n",
        "        self.system = system\n",
        "        graph = StateGraph(AgentState)\n",
        "        graph.add_node(\"llm\", self.call_openai)\n",
        "        graph.add_node(\"action\", self.take_action)\n",
        "        graph.add_conditional_edges(\n",
        "            \"llm\",\n",
        "            self.exists_action,\n",
        "            {True: \"action\", False: END}\n",
        "        )\n",
        "        graph.add_edge(\"action\", \"llm\")\n",
        "        graph.set_entry_point(\"llm\")\n",
        "        self.graph = graph.compile()\n",
        "        self.tools = {t.name: t for t in tools}\n",
        "        self.model = model.bind_tools(tools)\n",
        "\n",
        "    def exists_action(self, state: AgentState):\n",
        "        result = state['messages'][-1]\n",
        "        return len(result.tool_calls) > 0\n",
        "\n",
        "    def call_openai(self, state: AgentState):\n",
        "        messages = state['messages']\n",
        "        if self.system:\n",
        "            messages = [SystemMessage(content=self.system)] + messages\n",
        "        message = self.model.invoke(messages)\n",
        "        return {'messages': [message]}\n",
        "\n",
        "    def take_action(self, state: AgentState):\n",
        "        tool_calls = state['messages'][-1].tool_calls\n",
        "        results = []\n",
        "        for t in tool_calls:\n",
        "            print(f\"Calling: {t}\")\n",
        "            if not t['name'] in self.tools:      # check for bad tool name from LLM\n",
        "                print(\"\\n ....bad tool name....\")\n",
        "                result = \"bad tool name, retry\"  # instruct LLM to retry if bad\n",
        "            else:\n",
        "                result = self.tools[t['name']].invoke(t['args'])\n",
        "            results.append(ToolMessage(tool_call_id=t['id'], name=t['name'], content=str(result)))\n",
        "        print(\"Back to the model!\")\n",
        "        return {'messages': results}"
      ],
      "metadata": {
        "id": "UdU3VPenXgyG"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "prompt = \"\"\"You are a smart research assistant. Use the search engine to look up information. \\\n",
        "You are allowed to make multiple calls (either together or in sequence). \\\n",
        "Only look up information when you are sure of what you want. \\\n",
        "If you need to look up some information before asking a follow up question, you are allowed to do that!\n",
        "\"\"\"\n",
        "\n",
        "model = ChatOpenAI(model)\n",
        "abot = Agent(model, [tool], system=prompt)"
      ],
      "metadata": {
        "id": "Q4HWsShYX7WW",
        "outputId": "0ae7daee-da4a-49ea-b6cf-51eef79c3fb7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 193
        }
      },
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "error",
          "ename": "TypeError",
          "evalue": "BaseModel.__init__() takes 1 positional argument but 2 were given",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-34-0a44e5632403>\u001b[0m in \u001b[0;36m<cell line: 7>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      5\u001b[0m \"\"\"\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mChatOpenAI\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m \u001b[0mabot\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mAgent\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mtool\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msystem\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mprompt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mTypeError\u001b[0m: BaseModel.__init__() takes 1 positional argument but 2 were given"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "sLcZ7H8vYB5y"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}